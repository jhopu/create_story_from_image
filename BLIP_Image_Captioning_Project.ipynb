{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4985e2",
   "metadata": {},
   "source": [
    "# BLIP Image Captioning\n",
    "### Ashish Kumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97679d65",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "I chose this topic because it combines two powerful modalities—vision and language—into a single model. Understanding how models like BLIP generate text from visual input helped me appreciate the recent progress in multimodal learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd2bd9",
   "metadata": {},
   "source": [
    "## Historical Perspective on Multimodal Learning\n",
    "Multimodal learning has grown rapidly with models like CLIP, ViLBERT, and BLIP, which align image and text representations. These models have enabled tasks like image captioning, VQA, and retrieval with high accuracy, demonstrating the effectiveness of joint training on large image-text datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d17fb6",
   "metadata": {},
   "source": [
    "## Learning from the Project\n",
    "This project helped me understand how pretrained models work, how image and text embeddings are processed, and how to apply transformer-based architectures in practical applications like caption generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018281f",
   "metadata": {},
   "source": [
    "## Code Example\n",
    "The following Python snippet uses BLIP to generate a caption from a local image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load the image (update path accordingly)\n",
    "image_path = r\"C:\\Users\\ashish\\Downloads\\image.jpeg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocess and generate caption\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs, max_length=50)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3ab38",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "**What surprised me?** The quality and coherence of the captions generated by the model with zero fine-tuning.\n",
    "**Scope for improvement:** Fine-tuning BLIP on domain-specific datasets could improve performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16021a99",
   "metadata": {},
   "source": [
    "## References\n",
    "- [BLIP GitHub](https://github.com/salesforce/BLIP)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [Visual Haystacks Blog](https://bair.berkeley.edu/blog/2024/07/20/visual-haystacks/)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}